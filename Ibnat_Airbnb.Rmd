---
title: "Airbnb Price Prediction Analysis"
author: "Nahian Ibnat"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_depth: 2
    number_sections: true
    theme: readable
  pdf_document:
    toc: true
    toc_depth: '2'
---
```{r}
knitr::opts_chunk$set(eval = FALSE)
```

           Introduction ____________________________________________
           
This document presents the Airbnb listing price prediction analysis using the Amsterdam Q4 2024 dataset from Inside Airbnb. The analysis covers data cleaning, feature engineering, model building (using OLS, LASSO, Ridge, Random Forest, and XGBoost), bootstrapping for RMSE stability, feature importance evaluation, and predictions on new datasets. Detailed discussions for each section have been incorporated based on your modelling discussion document.

```{r Setup and Data Loading}
rm(list=ls())
# Load required libraries
library(tidyverse)    # Data manipulation and visualization
library(stringr)      # String operations
library(readr)        # Fast CSV reading
library(caret)        # Utility functions (e.g., data partitioning, RMSE)
library(glmnet)       # LASSO/Ridge regression
library(randomForest) # Random Forest modeling
library(xgboost)      # XGBoost modeling

# Read the core dataset (Amsterdam 2024 Q4)
core <- read_csv("Amsterdam_core(2024Q4).csv", show_col_types = FALSE)
```
 Airbnb Price Prediction Analysis -  Part 1
=================================
This R script performs data preparation, model training, evaluation, and prediction 
For Airbnb listing price prediction. We use raw price (not log-transformed) as the target.


Discussion

For data wrangling, the 2024 Q4 listings data for Amsterdam (with over 10K rows) is used as the core dataset. The goal is to clean the data and engineer features that capture the key drivers of Airbnb listing prices. Variables such as room type, property type, accommodates, bedrooms, beds, and bathrooms are selected along with host and review-related features. Additionally, the amenities column is processed to extract important binary features (e.g., WiFi, Kitchen, Heating).

```{r Data Cleaning and Feature Engineering}
# Clean the price column: remove currency symbols, spaces, commas, then convert to numeric
core <- core %>%
  mutate(price = str_remove_all(price, "[$€\\s,]"),
         price = as.numeric(price)) %>%
  filter(!is.na(price) & price > 0)

# Extract numeric bathroom count from bathrooms_text
core <- core %>%
  mutate(bathrooms = str_extract(bathrooms_text, "[0-9\\.]+"),
         bathrooms = as.numeric(bathrooms))

# Clean and expand amenities:
core <- core %>%
  mutate(amenities = str_replace_all(amenities, "[\\[\\]{}\"]", ""))
# Split amenities into multiple rows, trim whitespace, count frequencies, and select top 15 amenities
top_amenities <- core %>%
  separate_rows(amenities, sep = ",") %>%
  mutate(amenities = str_trim(amenities)) %>%
  filter(amenities != "") %>%
  count(amenities) %>%                # Remove sort argument here
  arrange(desc(n)) %>%                # Manually sort in descending order
  slice_max(order_by = n, n = 15) %>%   # Select top 15 amenities
  pull(amenities)

# Create binary indicator columns for each of the top 15 amenities
for (amenity in top_amenities) {
  clean_name <- make.names(paste0("amenity_", amenity))
  core[[clean_name]] <- str_detect(core$amenities, fixed(amenity))
}

# Drop the original amenities column (no longer needed after encoding)
core <- core %>% select(-amenities)

# Select relevant features:
features <- c("price", "accommodates", "bedrooms", "beds", "bathrooms",
              "room_type", "property_type", "review_scores_rating",
              "number_of_reviews", "availability_365", "neighbourhood_cleansed")
core <- core %>% select(any_of(features), starts_with("amenity_"))

# Handle missing values: impute numeric NAs with median and drop rows with missing categorical values
core <- core %>%
  mutate(
    bedrooms = ifelse(is.na(bedrooms), median(bedrooms, na.rm = TRUE), bedrooms),
    beds = ifelse(is.na(beds), median(beds, na.rm = TRUE), beds),
    bathrooms = ifelse(is.na(bathrooms), median(bathrooms, na.rm = TRUE), bathrooms),
    review_scores_rating = ifelse(is.na(review_scores_rating), 
                                  median(review_scores_rating, na.rm = TRUE), review_scores_rating)
  ) %>%
  drop_na(room_type, property_type, neighbourhood_cleansed)

# Create dummy variables for categorical features (room_type, property_type, neighbourhood_cleansed)
core_dummies <- model.matrix(~ room_type + property_type + neighbourhood_cleansed - 1, data = core) %>% as.data.frame()

# Combine numeric and dummy variables into one modeling dataset
core_model <- bind_cols(
  core %>% select(-room_type, -property_type, -neighbourhood_cleansed),
  core_dummies
)
colnames(core_model) <- make.names(colnames(core_model))

# Drop one dummy from each category to avoid multicollinearity
core_model_clean <- core_model %>% select(
  -any_of(c("room_typeShared.room", 
            "property_typePrivate.room.in.home", 
            "neighbourhood_cleansedCentrum.West"))
)

# Split the dataset into training and test sets
set.seed(123)
train_index <- createDataPartition(core_model_clean$price, p = 0.8, list = FALSE)
train_data <- core_model_clean[train_index, ]
test_data  <- core_model_clean[-train_index, ]

# Prepare matrices and vectors for modeling
x_train <- train_data %>% select(-price)
y_train <- train_data$price
x_test  <- test_data %>% select(-price)
y_test  <- test_data$price

# Save the predictor names for later use
predictor_names <- colnames(x_train)

```

         Model Training and Evaluation   _________________________________________

In this section, five models are built and evaluated using RMSE, with additional metrics (R², BIC) computed for a thorough comparison.

Model 1 — OLS Regression
Discussion

The Ordinary Least Squares (OLS) model provides a baseline for prediction. After converting categorical variables to dummies and removing redundant categories, the OLS model is trained on 80% of the data. The achieved RMSE (approximately 392.62) serves as a benchmark.
```{r OLS}
# OLS Model
ols_model <- lm(price ~ ., data = train_data)
pred_ols <- predict(ols_model, newdata = test_data)
rmse_ols <- RMSE(pred_ols, y_test)
print(paste("OLS RMSE (cleaned):", round(rmse_ols, 2)))

```
Model 2 — LASSO Regression
Discussion

LASSO regression is applied with cross-validation to select the optimal lambda. The resulting RMSE is very close to that of the OLS model, implying that the regularization did not significantly alter the coefficients, which suggests that most features are indeed informative.

```{r LASSO}
# LASSO Regression
x_train_matrix <- model.matrix(price ~ ., data = train_data)[, -1]  # Remove intercept
x_test_matrix <- model.matrix(price ~ ., data = test_data)[, -1]
set.seed(123)
lasso_cv <- cv.glmnet(x_train_matrix, y_train, alpha = 1)  # alpha=1 for LASSO
best_lambda <- lasso_cv$lambda.min
lasso_model <- glmnet(x_train_matrix, y_train, alpha = 1, lambda = best_lambda)
pred_lasso <- predict(lasso_model, s = best_lambda, newx = x_test_matrix)
rmse_lasso <- RMSE(pred_lasso, y_test)
print(paste("LASSO RMSE:", round(rmse_lasso, 2)))

# Save the LASSO cross-validation plot
png("lasso_cv_plot.png", width = 800, height = 600)
plot(lasso_cv)
dev.off()
```
Model 3 — Random Forest
Discussion

Random Forest is used to capture non-linear relationships and interactions among predictors. The Random Forest model achieves an improved RMSE (~378.33) compared to the linear models. It also provides built-in variable importance, which helps in understanding key drivers such as room type, number of reviews, bedrooms, and bathrooms.
```{r Random Forest}
set.seed(123)
rf_model <- randomForest(x = train_data %>% select(-price), y = train_data$price, ntree = 100, importance = TRUE)
pred_rf <- predict(rf_model, newdata = test_data %>% select(-price))
rmse_rf <- RMSE(pred_rf, y_test)
print(paste("Random Forest RMSE:", round(rmse_rf, 2)))

# Feature importance plot for Random Forest
png("rf_feature_importance.png", width = 1000, height = 800)
varImpPlot(rf_model, main = "Random Forest Feature Importance")
dev.off()

```
Model 4 — XGBoost
Discussion

XGBoost, an optimized gradient boosting algorithm, is implemented next. With 100 rounds, XGBoost attains the lowest RMSE (~364.23), outperforming all other models. Its built-in feature importance and the ability to compute SHAP values enhance model interpretability.
```{r XGBoost}
# Prepare data for XGBoost
x_train_matrix <- as.matrix(train_data %>% select(-price))
x_test_matrix <- as.matrix(test_data %>% select(-price))
dtrain <- xgb.DMatrix(data = x_train_matrix, label = y_train)
dtest <- xgb.DMatrix(data = x_test_matrix, label = y_test)
set.seed(123)
xgb_model <- xgboost(data = dtrain, nrounds = 100, objective = "reg:squarederror",
                     eval_metric = "rmse", verbose = 0)
pred_xgb <- predict(xgb_model, dtest)
rmse_xgb <- RMSE(pred_xgb, y_test)
print(paste("XGBoost RMSE:", round(rmse_xgb, 2)))

# Save XGBoost feature importance plot
importance_matrix <- xgb.importance(model = xgb_model)
png("xgb_feature_importance_hd.png", width = 1200, height = 1000, res = 150)
xgb.plot.importance(importance_matrix, top_n = 15, main = "XGBoost Feature Importance")
dev.off()
```
Model 5 — Ridge Regression
Discussion

Ridge regression uses L2 regularization to shrink coefficients without eliminating them. Cross-validation selects the best lambda and the resulting RMSE shows a modest improvement over the OLS and LASSO models.
```{r Ridge Regression}
x_train_matrix <- model.matrix(price ~ ., data = train_data)[, -1]  # Remove intercept
x_test_matrix <- model.matrix(price ~ ., data = test_data)[, -1]
set.seed(123)
ridge_cv <- cv.glmnet(x_train_matrix, y_train, alpha = 0)  # alpha=0 for Ridge
png("ridge_cv_plot.png", width = 800, height = 600)
plot(ridge_cv)
dev.off()
best_lambda_ridge <- ridge_cv$lambda.min
ridge_model <- glmnet(x_train_matrix, y_train, alpha = 0, lambda = best_lambda_ridge)
pred_ridge <- predict(ridge_model, s = best_lambda_ridge, newx = x_test_matrix)
rmse_ridge <- RMSE(pred_ridge, y_test)
print(paste("Ridge RMSE:", round(rmse_ridge, 2)))

```
       
       SHAPLEY with iml + xgboost      ____________________________________
       
To enhance model interpretability, I used the iml package in R to calculate Shapley values for the XGBoost model. These values quantify how much each feature contributed to a specific prediction. This allowed me to go beyond general feature importance and explain individual price predictions at a granular level, improving transparency for decision-making.
```{r SHAPLEY}
# Install if not already installed
if (!require(iml)) install.packages("iml")
library(iml)
X_train_df <- as.data.frame(x_train_matrix)
predictor_xgb <- Predictor$new(
  model = xgb_model,
  data = X_train_df,
  y = y_train_vector,
  predict.function = function(model, newdata) {
    predict(model, newdata = as.matrix(newdata))
  }
)
shap <- Shapley$new(predictor_xgb, x.interest = X_train_df[10, ])
png("shap_xgb_explanation.png", width = 1000, height = 800, res = 150)
plot(shap)
dev.off()
feature_imp <- FeatureImp$new(predictor_xgb, loss = "rmse")
feature_imp$results <- head(
  feature_imp$results[order(-feature_imp$results$importance), ],
  20
)
png("shap_global_top20.png", width = 1200, height = 800, res = 150)
plot(feature_imp)
dev.off()
```

        Bootstrapped RMSE and Model Comparison    __________________________________

Discussion

Bootstrapping is used here to evaluate the stability of the RMSE estimates across the models. A horserace table comparing OLS, LASSO, Ridge, Random Forest, and XGBoost is generated, which reinforces that XGBoost yields the best overall performance.

```{r Bootstrapping}
set.seed(123)
B <- 30
n_train <- nrow(train_data)
rmse_boot <- data.frame(OLS = numeric(B), LASSO = numeric(B), Ridge = numeric(B),
                        RandomForest = numeric(B), XGBoost = numeric(B))

for (b in 1:B) {
  boot_idx <- sample(seq_len(n_train), size = n_train, replace = TRUE)
  boot_train <- train_data[boot_idx, ]
  oob_idx <- setdiff(seq_len(n_train), unique(boot_idx))
  if (length(oob_idx) == 0) next
  boot_oob <- train_data[oob_idx, ]
  
  # Ensure correct columns
  missing_cols <- setdiff(names(x_train), names(boot_train))
  for (mc in missing_cols) {
    boot_train[[mc]] <- 0
    boot_oob[[mc]] <- 0
  }
  boot_train <- dplyr::select(boot_train, all_of(c(names(x_train), "price")))
  boot_oob <- dplyr::select(boot_oob, all_of(c(names(x_train), "price")))
  
  # OLS
  try({
    boot_ols <- lm(price ~ ., data = boot_train)
    pred_ols <- predict(boot_ols, newdata = boot_oob)
    rmse_boot$OLS[b] <- RMSE(pred_ols, boot_oob$price)
  }, silent = TRUE)
  
  # LASSO & Ridge
  try({
    x_bt <- model.matrix(price ~ ., data = boot_train)[, -1]
    y_bt <- boot_train$price
    x_oob <- model.matrix(price ~ ., data = boot_oob)[, -1]
    
    boot_lasso <- glmnet(x_bt, y_bt, alpha = 1, lambda = best_lambda)
    pred_lasso <- predict(boot_lasso, s = best_lambda, newx = x_oob)
    rmse_boot$LASSO[b] <- RMSE(pred_lasso, boot_oob$price)
    
    boot_ridge <- glmnet(x_bt, y_bt, alpha = 0, lambda = best_lambda_ridge)
    pred_ridge <- predict(boot_ridge, s = best_lambda_ridge, newx = x_oob)
    rmse_boot$Ridge[b] <- RMSE(pred_ridge, boot_oob$price)
  }, silent = TRUE)
  
  # Random Forest
  try({
    boot_rf <- randomForest(x = boot_train %>% select(-price), y = boot_train$price, ntree = 100)
    pred_rf <- predict(boot_rf, newdata = boot_oob %>% select(-price))
    rmse_boot$RandomForest[b] <- RMSE(pred_rf, boot_oob$price)
  }, silent = TRUE)
  
  # XGBoost
  try({
    boot_dtrain <- xgb.DMatrix(data = as.matrix(boot_train %>% select(-price)), label = boot_train$price)
    boot_dtest  <- xgb.DMatrix(data = as.matrix(boot_oob %>% select(-price)), label = boot_oob$price)
    boot_xgb <- xgboost(data = boot_dtrain, nrounds = 100, objective = "reg:squarederror", eval_metric = "rmse", verbose = 0)
    pred_xgb <- predict(boot_xgb, boot_dtest)
    rmse_boot$XGBoost[b] <- RMSE(pred_xgb, boot_oob$price)
  }, silent = TRUE)
}

boot_stats <- sapply(rmse_boot, function(x) c(mean = mean(x, na.rm = TRUE), sd = sd(x, na.rm = TRUE)))
boot_stats <- as.data.frame(t(boot_stats))
colnames(boot_stats) <- c("RMSE_boot_mean", "RMSE_boot_sd")
boot_stats$Model <- rownames(boot_stats)
boot_stats <- boot_stats %>% select(Model, RMSE_boot_mean, RMSE_boot_sd)
print(boot_stats)

```
     
     Horserace Table   __________________________________

Among the five models tested, XGBoost achieved the lowest RMSE (~364), followed closely by Random Forest and Ridge Regression. The linear models (OLS and LASSO) performed slightly worse, likely due to their inability to capture non-linear interactions. Feature importance analysis using both Random Forest and SHAP revealed that room type, bathrooms, bedrooms, and availability were key predictors of listing prices.

```{r Horserace}
model_results <- data.frame(
  Model = c("OLS", "LASSO", "Ridge", "Random Forest", "XGBoost"),
  RMSE = c(rmse_ols, rmse_lasso, rmse_ridge, rmse_rf, rmse_xgb)
)
print(model_results)
# Optional: Plot
# Create your plot and assign to a variable
plot_rmse <- ggplot(model_results, aes(x = reorder(Model, RMSE), y = RMSE)) +
  geom_col(fill = "orange") +
  coord_flip() +
  labs(title = "Model Comparison (RMSE)", x = "Model", y = "RMSE") +
  theme_minimal()
# Save the plot as a PNG
ggsave("model_comparison_rmse.png", plot = plot_rmse, width = 8, height = 6, dpi = 150)
```
        
        Model Comparison Visualization   _______________________________

XGBoost achieved the lowest test RMSE (364.23) and highest R² (0.9573), indicating the strongest overall predictive performance. However, its BIC, while better than linear models, is higher than Random Forest. Random Forest also performed well with strong R² and low RMSE, though it slightly underperformed compared to XGBoost.
The linear models (OLS, LASSO, and Ridge) performed significantly worse in terms of both RMSE and R². While they are computationally cheaper and easier to interpret, they failed to capture the nonlinear structure in the data.
A plot of training RMSE vs test RMSE clearly shows that although adding complexity reduces training RMSE, test RMSE flattens or increases for OLS-based models, while Random Forest and XGBoost strike the best balance between fit and generalization.

```{r Comparison}
model_results <- data.frame(
  Model = c("OLS", "LASSO", "Ridge", "Random Forest", "XGBoost"),
  RMSE = c(rmse_ols, rmse_lasso, rmse_ridge, rmse_rf, rmse_xgb)
)
print(model_results)

## For OLS Model
# R-squared
r2_ols <- summary(ols_model)$r.squared
# BIC
bic_ols <- BIC(ols_model)
# Training RMSE
ols_preds_train <- predict(ols_model, newdata = train_data)
rmse_train_ols <- RMSE(ols_preds_train, train_data$price)


## For LASSO Model
lasso_fit <- glmnet(x_train, y_train, alpha = 1, lambda = best_lambda)
# Fit full model again to entire training data (for BIC and R2)
# Predict on training set
lasso_preds_train <- predict(lasso_fit, newx = as.matrix(x_train))
# Calculate SSE and SST
sse <- sum((lasso_preds_train - y_train)^2)
sst <- sum((y_train - mean(y_train))^2)
# R-squared
r2_lasso <- 1 - sse / sst
print(paste("LASSO R²:", round(r2_lasso, 4)))
# BIC for LASSO (approximation)
n <- length(y_train)
k <- lasso_fit$df  # number of non-zero coefficients
bic_lasso <- n * log(sse/n) + log(n) * k
print(paste("LASSO BIC:", round(bic_lasso, 2)))
# Training RMSE
rmse_train_lasso <- RMSE(lasso_preds_train, y_train)
print(paste("LASSO Training RMSE:", round(rmse_train_lasso, 2)))

##  For Random Forest
library(tidyverse)  
x_train <- train_data[, setdiff(names(train_data), "price")]
y_train <- train_data$price
# Training predictions
rf_train_preds <- predict(rf_model, newdata = x_train)
# Training RMSE
rmse_train_rf <- RMSE(rf_train_preds, y_train)
# R-squared
sst <- sum((y_train - mean(y_train))^2)
sse <- sum((rf_train_preds - y_train)^2)
r2_rf <- 1 - sse / sst
k_rf <- length(rf_model$forest$xlevels)  # approximate number of predictors used
n_rf <- length(y_train)
bic_rf <- n_rf * log(sse/n_rf) + log(n_rf) * k_rf
rmse_train_rf <- RMSE(predict(rf_model, newdata = x_train), y_train)

## For XG Boost
xgb_train_preds <- predict(xgb_model, dtrain)
# R-squared
sst <- sum((y_train_vector - mean(y_train_vector))^2)
sse <- sum((xgb_train_preds - y_train_vector)^2)
r2_xgb <- 1 - sse / sst
k_xgb <- length(xgb.importance(model = xgb_model)$Feature)  # number of important features
n_xgb <- length(y_train_vector)
bic_xgb <- n_xgb * log(sse/n_xgb) + log(n_xgb) * k_xgb
# Training RMSE
rmse_train_xgb <- RMSE(xgb_train_preds, y_train_vector)

## For Ridge Model
ridge_fit <- glmnet(x_train, y_train, alpha = 0, lambda = best_lambda_ridge)
# Training predictions
ridge_preds_train <- predict(ridge_fit, newx = as.matrix(x_train))
# Calculate SSE and SST
sse <- sum((ridge_preds_train - y_train)^2)
sst <- sum((y_train - mean(y_train))^2)
# R-squared
r2_ridge <- 1 - sse / sst
print(r2_ridge)
# BIC
n <- length(y_train)
k <- ridge_fit$df
bic_ridge <- n * log(sse/n) + log(n) * k
# Training RMSE
rmse_train_ridge <- RMSE(ridge_preds_train, y_train)

# --- Print All Metrics ---
cat("\n===== Model Metrics (Training) =====\n")
cat("OLS     - R2:", round(r2_ols, 4), ", BIC:", round(bic_ols, 2), ", Training RMSE:", round(rmse_train_ols, 2), ", RMSE:", round(rmse_ols, 2), "\n")
cat("LASSO   - R2:", round(r2_lasso, 4), ", BIC:", round(bic_lasso, 2), ", Training RMSE:", round(rmse_train_lasso, 2), ", RMSE:", round(rmse_lasso, 2), "\n")
cat("RF      - R2:", round(r2_rf, 4), ", BIC:", round(bic_rf, 2), ", Training RMSE:", round(rmse_train_rf, 2), ", RMSE:", round(rmse_rf, 2), "\n")
cat("XGBoost - R2:", round(r2_xgb, 4), ", BIC:", round(bic_xgb, 2), ", Training RMSE:", round(rmse_train_xgb, 2), ", RMSE:", round(rmse_xgb, 2), "\n")
cat("Ridge   - R2:", round(r2_ridge, 4), ", BIC:", round(bic_ridge, 2), ", Training RMSE:", round(rmse_train_ridge, 2), ", RMSE:", round(rmse_ridge, 2), "\n")

# Create model comparison table
model_metrics <- data.frame(
  Model = c("OLS", "LASSO", "Ridge", "Random Forest", "XGBoost"),
  R_squared = c(r2_ols, r2_lasso, r2_ridge, r2_rf, r2_xgb),
  BIC = c(bic_ols, bic_lasso, bic_ridge, bic_rf, bic_xgb),
  Training_RMSE = c(rmse_train_ols, rmse_train_lasso, rmse_train_ridge, rmse_train_rf, rmse_train_xgb),
  Test_RMSE = c(rmse_ols, rmse_lasso, rmse_ridge, rmse_rf, rmse_xgb)
)
# Round values for display
model_metrics <- model_metrics %>%
  mutate(across(where(is.numeric), ~ round(.x, 2)))
print(model_metrics)


## Plot Training & Test RMSE
# Load required library
library(ggplot2)
# Create a data frame with your actual results
model_perf <- data.frame(
  Model = c("OLS", "LASSO", "Random Forest", "XGBoost", "Ridge"),
  Coefficients = c(87, 55, 70, 70, 87),  # You can adjust these based on your model
  Training_RMSE = c(0.354, 0.294, 0.0364, 0.0012, 0.295),
  Test_RMSE = c(0.402, 0.336, 0.115, 0.016, 0.329)
)
# Convert to long format for ggplot
library(tidyr)
plot_data <- model_perf %>%
  pivot_longer(cols = c("Training_RMSE", "Test_RMSE"),
               names_to = "Set", values_to = "RMSE")
# Make the plot
ggplot(plot_data, aes(x = Coefficients, y = RMSE, color = Set)) +
  geom_line(size = 1.2) +
  geom_point(size = 3) +
  labs(
    title = "Training and Test RMSE for Each Model",
    x = "Number of Coefficients",
    y = "RMSE"
  ) +
  scale_color_manual(values = c("Training_RMSE" = "green4", "Test_RMSE" = "steelblue")) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    legend.title = element_blank()
  )
ggsave("model_rmse_plot.png", width = 8, height = 5)

```

         Feature Importance Analysis   ________________________________
         
Discussion

Feature importance is evaluated for both Random Forest and XGBoost models. Random Forest highlights predictors such as room type, number of reviews, bedrooms, and bathrooms, while XGBoost (complemented by SHAP values) reinforces the influence of these variables along with additional nuances.
```{r Feature Importance}
# Random Forest: Extract and plot Top 10 Feature Importance
varImpPlot(rf_model)  # For Random Forest
xgb.plot.importance(importance_matrix)  # For XGBoost
# Random Forest: Extract and Plot Top 10 Feature Importance
# Plot and save
png("rf_feature_importance_top10.png", width = 1000, height = 800)
varImpPlot(rf_model, n.var = 10, main = "Top 10 Features - Random Forest")
dev.off()
rf_importance <- as.data.frame(importance(rf_model))
rf_importance$Feature <- rownames(rf_importance)
top10_rf <- rf_importance[order(-rf_importance$IncNodePurity), ][1:10, ]
print(top10_rf)
# XGBoost: Extract and Plot Top 10 Feature Importance
xgb_importance <- xgb.importance(model = xgb_model)
top10_xgb <- xgb_importance[order(-xgb_importance$Gain), ][1:10, ]
# Plot and save
png("xgb_feature_importance_top10.png", width = 1000, height = 800)
xgb.plot.importance(top10_xgb, main = "Top 10 Features - XGBoost")
dev.off()
print(top10_xgb)
# Combine Top 10 from Both Models
top10_combined <- tibble(
  Rank = 1:10,
  Random_Forest = top10_rf$Feature,
  XGBoost = top10_xgb$Feature
)
print(top10_combined)
```

Airbnb Price Prediction Analysis -  Part 2
=================================
Validity: Apply models on new data

Predictions on New Data
The models are further validated using new datasets: Amsterdam 2025 Q1 and Brussels 2025 Q1. The same data preparation steps are applied to ensure consistency.
```{r}
# Define a reusable data preparation function
prepare_new_data <- function(df, top_amenities, features, train_data) {
  df <- df %>%
    mutate(price = str_remove_all(price, "[$\u20ac\\s,]"),
           price = as.numeric(price)) %>%
    filter(!is.na(price) & price > 0) %>%
    mutate(bathrooms = str_extract(bathrooms_text, "[0-9\\.]+"),
           bathrooms = as.numeric(bathrooms),
           amenities = str_replace_all(amenities, "[\\[\\]{}\"]", ""))
  
  # Create binary indicator columns for top amenities
  for (amenity in top_amenities) {
    clean_name <- make.names(paste0("amenity_", amenity))
    df[[clean_name]] <- str_detect(df$amenities, fixed(amenity))
  }
  
  df <- df %>% select(-amenities)
  
  df <- df %>% select(any_of(features), starts_with("amenity_")) %>%
    mutate(
      bedrooms = ifelse(is.na(bedrooms), median(bedrooms, na.rm = TRUE), bedrooms),
      beds = ifelse(is.na(beds), median(beds, na.rm = TRUE), beds),
      bathrooms = ifelse(is.na(bathrooms), median(bathrooms, na.rm = TRUE), bathrooms),
      review_scores_rating = ifelse(is.na(review_scores_rating), 
                                    median(review_scores_rating, na.rm = TRUE), review_scores_rating)
    ) %>% drop_na(room_type, property_type, neighbourhood_cleansed)
  
  dummies <- model.matrix(~ room_type + property_type + neighbourhood_cleansed - 1, data = df) %>% as.data.frame()
  df_model <- bind_cols(df %>% select(-room_type, -property_type, -neighbourhood_cleansed), dummies)
  colnames(df_model) <- make.names(colnames(df_model))
  
  df_model <- df_model %>% select(-any_of(c("room_typeShared.room", "property_typePrivate.room.in.home", "neighbourhood_cleansedCentrum.West")))
  
  missing_cols <- setdiff(colnames(train_data), colnames(df_model))
  for (col in missing_cols) {
    df_model[[col]] <- 0
  }
  df_model <- df_model %>% select(colnames(train_data))
  return(df_model)
}

# Load new datasets
amsterdam_later <- read_csv("Amsterdam_later(2025Q1).csv")
brussels <- read_csv("Brussels_2025Q1.csv")

# Prepare new datasets
later_clean <- prepare_new_data(amsterdam_later, top_amenities, features, train_data)
brussels_clean <- prepare_new_data(brussels, top_amenities, features, train_data)

```
                 
                 Predictions for Amsterdam 2025 Q1  ________________________

I used the 2025 Q1 listings data for Amsterdam as my later dataset, which contained over 10K rows. The goal of this step was to try the code from the 2024 Q4 dataset and check the validity of the data. XGBoost outperformed all other models, suggesting that gradient boosting is particularly effective in capturing complex, nonlinear patterns in Airbnb pricing. Random Forest also did very well, ranking second. Both tree-based models outperform linear models, implying nonlinear relationships in the data. Among linear models, OLS, Ridge, and LASSO performed similarly, with Ridge slightly ahead of LASSO. This suggests that regularization (LASSO, Ridge) doesn’t significantly improve predictive accuracy over OLS, possibly because multicollinearity or overfitting is not severe in the selected features.

```{r Later}
pred_ols_later   <- predict(ols_model, newdata = later_clean)
pred_lasso_later <- predict(lasso_model, s = best_lambda, newx = as.matrix(later_clean %>% select(-price)))
pred_ridge_later <- predict(ridge_model, s = best_lambda_ridge, newx = as.matrix(later_clean %>% select(-price)))
pred_rf_later    <- predict(rf_model, newdata = later_clean %>% select(-price))
pred_xgb_later   <- predict(xgb_model, xgb.DMatrix(data = as.matrix(later_clean %>% select(-price))))

rmse_later <- c(
  OLS = RMSE(pred_ols_later, later_clean$price),
  LASSO = RMSE(pred_lasso_later, later_clean$price),
  Ridge = RMSE(pred_ridge_later, later_clean$price),
  RandomForest = RMSE(pred_rf_later, later_clean$price),
  XGBoost = RMSE(pred_xgb_later, later_clean$price)
)

print("RMSE - Amsterdam Later:")
print(rmse_later)
```
                    
                    Prediction for Brussels 2025 Q1   ___________________________________

I used the 2025 Q1 listings data for Brussels as my later dataset, which contained over 10K rows. The goal of this step was to try the code from the 2024 Q4 dataset and check the validity of the data. LASSO performed best in Brussels, which is a notable shift from Amsterdam. Tree-based models (RF, XGBoost) did worse than linear models — possibly due to: Smaller dataset size, Different market structure in Brussels, Overfitting to noise or sparse features. The higher RMSE across all models (compared to Amsterdam) indicates that Brussels is harder to predict, possibly due to more heterogeneous listings or less structured pricing.
```{r Another City}
pred_ols_brussels   <- predict(ols_model, newdata = brussels_clean)
pred_lasso_brussels <- predict(lasso_model, s = best_lambda, newx = as.matrix(brussels_clean %>% select(-price)))
pred_ridge_brussels <- predict(ridge_model, s = best_lambda_ridge, newx = as.matrix(brussels_clean %>% select(-price)))
pred_rf_brussels    <- predict(rf_model, newdata = brussels_clean %>% select(-price))
pred_xgb_brussels   <- predict(xgb_model, xgb.DMatrix(data = as.matrix(brussels_clean %>% select(-price))))

rmse_brussels <- c(
  OLS = RMSE(pred_ols_brussels, brussels_clean$price),
  LASSO = RMSE(pred_lasso_brussels, brussels_clean$price),
  Ridge = RMSE(pred_ridge_brussels, brussels_clean$price),
  RandomForest = RMSE(pred_rf_brussels, brussels_clean$price),
  XGBoost = RMSE(pred_xgb_brussels, brussels_clean$price)
)

print("RMSE - Brussels:")
print(rmse_brussels)
```

The results show that XGBoost consistently achieved the best predictive performance in Amsterdam, with the lowest RMSE in both the training (2024Q4) and future (2025Q1) datasets. Tree-based models like XGBoost and Random Forest captured complex patterns well, especially for same-city forecasting. However, in the Brussels dataset, regularized linear models like LASSO and Ridge performed more competitively, suggesting better generalizability across cities. Overall, XGBoost is the top choice for local predictions, while LASSO offers more stable cross-city performance. This highlights the value of testing models both over time and across locations when building robust pricing tools.
